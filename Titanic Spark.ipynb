{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this script we will train a model for the titanic dataset. This is an exercise that is done for \n",
    "many people who is introduced in the Machine Learning analysis. I have done this before with sklearn, pandas, and other common libraries for data analysis, but today I will make the same thing with spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder\\\n",
    ".master(\"local[*]\")\\\n",
    ".appName(\"Titanic_model\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are probably asking what \"local[*]\" means. We pass this parameter to the method master, and it means that we´re Running Spark locally with as many worker threads as logical cores on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Titanic=spark.read.csv(\"Titanic.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------------------+------+------+-----+-----+------+--------+-------+--------+----+----+--------------------+\n",
      "|pclass|survived|                name|   sex|   age|sibsp|parch|ticket|    fare|  cabin|embarked|boat|body|           home_dest|\n",
      "+------+--------+--------------------+------+------+-----+-----+------+--------+-------+--------+----+----+--------------------+\n",
      "|     1|       1|Allen, Miss. Elis...|female|  29.0|    0|    0| 24160|211.3375|     B5|       S|   2|null|        St Louis, MO|\n",
      "|     1|       1|Allison, Master. ...|  male|0.9167|    1|    2|113781|  151.55|C22 C26|       S|  11|null|Montreal, PQ / Ch...|\n",
      "|     1|       0|Allison, Miss. He...|female|   2.0|    1|    2|113781|  151.55|C22 C26|       S|null|null|Montreal, PQ / Ch...|\n",
      "|     1|       0|Allison, Mr. Huds...|  male|  30.0|    1|    2|113781|  151.55|C22 C26|       S|null| 135|Montreal, PQ / Ch...|\n",
      "+------+--------+--------------------+------+------+-----+-----+------+--------+-------+--------+----+----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Titanic.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fisrt we need to do a clean data work. We´re going to see the variables, null values, how we will impute them, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that if we select the home.dest column, we obtain an error, that implies that probably\n",
    "we need to change the name of the column, let´s see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to decide what columns mantain in the analysis, so then we won´t make work that doesnt is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+---+---+-----+-----+------+----+-----+--------+----+----+---------+\n",
      "|pclass|survived|name|sex|age|sibsp|parch|ticket|fare|cabin|embarked|boat|body|home_dest|\n",
      "+------+--------+----+---+---+-----+-----+------+----+-----+--------+----+----+---------+\n",
      "|     0|       0|   0|  0|263|    0|    0|     0|   1| 1014|       2| 823|1188|      564|\n",
      "+------+--------+----+---+---+-----+-----+------+----+-----+--------+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Titanic.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in Titanic.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are a lot of null values in body,boat,cabin,home_dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns=[\"age\",\"sibsp\",\"parch\",\"fare\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-----------------+\n",
      "|summary|               age|             sibsp|             parch|             fare|\n",
      "+-------+------------------+------------------+------------------+-----------------+\n",
      "|  count|              1046|              1309|              1309|             1308|\n",
      "|   mean|  29.8811345124283|0.4988540870893812|0.3850267379679144|33.29547928134572|\n",
      "| stddev|14.413499699923596|1.0416583905961012|0.8655602753495143|51.75866823917421|\n",
      "|    min|            0.1667|                 0|                 0|              0.0|\n",
      "|    25%|              21.0|                 0|                 0|           7.8958|\n",
      "|    50%|              28.0|                 0|                 0|          14.4542|\n",
      "|    75%|              39.0|                 1|                 0|           31.275|\n",
      "|    max|              80.0|                 8|                 9|         512.3292|\n",
      "+-------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Titanic.select(numerical_columns).summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train,titanic_test=Titanic.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s imputte the variables like this:\n",
    "- Age\n",
    "\n",
    "Impute the age with the mean of the pclass of each row.\n",
    "\n",
    "- Cabin\n",
    "\n",
    "Fill with an Unknown, and only place the letter of the cabin, since there are a lot.\n",
    "\n",
    "- Embarked\n",
    "\n",
    "The most frecuent value\n",
    "\n",
    "\n",
    "- fare\n",
    "\n",
    "It´s 1 value only, the mean of all the train data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_variable(data):\n",
    "    Dictionary_means=data.select([\"age\",\"pclass\"]).groupBy(\"pclass\").mean()\n",
    "    data=data.join(Dictionary_means.select([\"pclass\",\"avg(age)\"]),on=\"pclass\",how=\"left\")\n",
    "    # Round the avg(column)\n",
    "    data=data.withColumn(\"avg(age)\",f.round(data[\"avg(age)\"],0))\n",
    "    # Replace the null values with the mean of the class\n",
    "    data=data.withColumn(\"age\",f.when(data[\"age\"].isNull(),data[\"avg(age)\"]).otherwise(Titanic[\"age\"]))\n",
    "    ### Cabin\n",
    "    udf=UserDefinedFunction(lambda x: x[0],StringType())\n",
    "    # Fill the null values with an U string\n",
    "    data=data.withColumn(\"cabin\",f.when(data[\"cabin\"].isNull(),\"U\").otherwise(data[\"cabin\"]))\n",
    "    # Select only the letter of the boat\n",
    "    data=data.withColumn(\"cabin\",udf(data[\"cabin\"]))\n",
    "    ### Embarked: Replace with the most common port of embarcation S\n",
    "    data=data.withColumn(\"embarked\",f.when(data[\"embarked\"].isNull(),\"S\").otherwise(data[\"embarked\"]))\n",
    "    ## Fare\n",
    "    mean_fare=data.agg(f.avg(f.col(\"fare\"))).collect()[0][0]\n",
    "    data=data.withColumn(\"fare\",f.when(data[\"fare\"].isNull(),mean_fare).otherwise(data[\"fare\"]))\n",
    "    # Now let´s drop the useless columns\n",
    "    data=data.drop(*[\"body\",\"home_dest\",\"boat\",\"name\",\"ticket\",\"avg(age)\"])\n",
    "    return(data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Why did we create this function instead of apply to the full Titanic dataset?\n",
    "Because we need to separate completly the data train and data test, and test data must not influence\n",
    "to the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train=impute_variable(titanic_train)\n",
    "titanic_test=impute_variable(titanic_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Titanic=impute_variable(Titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+----+-----+-----+--------+-----+--------+\n",
      "|pclass|survived|   sex| age|sibsp|parch|    fare|cabin|embarked|\n",
      "+------+--------+------+----+-----+-----+--------+-----+--------+\n",
      "|     1|       0|  male|42.0|    0|    0|   26.55|    U|       S|\n",
      "|     1|       0|female|25.0|    1|    2|  151.55|    C|       S|\n",
      "|     1|       0|  male|71.0|    0|    0| 49.5042|    U|       C|\n",
      "|     1|       0|  male|47.0|    1|    0| 227.525|    C|       C|\n",
      "|     1|       0|  male|24.0|    0|    1|247.5208|    B|       C|\n",
      "|     1|       0|  male|36.0|    0|    0| 75.2417|    C|       C|\n",
      "|     1|       0|  male|25.0|    0|    0|    26.0|    U|       C|\n",
      "|     1|       0|  male|41.0|    0|    0|    30.5|    A|       S|\n",
      "|     1|       0|  male|48.0|    0|    0| 50.4958|    B|       C|\n",
      "|     1|       0|  male|45.0|    0|    0|   26.55|    B|       S|\n",
      "|     1|       0|  male|33.0|    0|    0|     5.0|    B|       S|\n",
      "|     1|       0|  male|49.0|    0|    0|    26.0|    U|       S|\n",
      "|     1|       0|  male|36.0|    1|    0|   78.85|    C|       S|\n",
      "|     1|       0|  male|38.0|    0|    0|     0.0|    U|       S|\n",
      "|     1|       0|  male|27.0|    1|    0|136.7792|    C|       C|\n",
      "|     1|       0|  male|38.0|    0|    0|    52.0|    A|       S|\n",
      "|     1|       0|  male|47.0|    0|    0| 25.5875|    E|       S|\n",
      "|     1|       0|  male|37.0|    1|    1| 83.1583|    E|       C|\n",
      "|     1|       0|  male|38.0|    0|    0|   26.55|    U|       S|\n",
      "|     1|       0|  male|70.0|    1|    1|    71.0|    B|       S|\n",
      "+------+--------+------+----+-----+-----+--------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# te amo fer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have this, we need to convert the categorical columms to integers and then apply OneHotEncoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pclass',\n",
       " 'survived',\n",
       " 'sex',\n",
       " 'age',\n",
       " 'sibsp',\n",
       " 'parch',\n",
       " 'fare',\n",
       " 'cabin',\n",
       " 'embarked']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns=[\"pclass\",\"sex\",\"cabin\",\"embarked\"]\n",
    "numerical_columns=[\"age\",\"sibsp\",\"parch\",\"fare\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder as OneHotEncoderEstimator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to train the OHE with all the dataset, since then of spplitting the data in train,test\n",
    "sometimes it can be that are columns with more categorical values in some of the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SI=[StringIndexer(inputCol=column,outputCol=column+\"_num\").fit(Titanic) for column in categorical_columns]\n",
    "categorical_columns_num=[column+\"_num\" for column in categorical_columns]\n",
    "categorical_columns_OHE=[column+\"_OHE\" for column in categorical_columns]\n",
    "OHE=OneHotEncoderEstimator(inputCols=categorical_columns_num,outputCols=categorical_columns_OHE)\n",
    "VA=VectorAssembler(inputCols=numerical_columns+OHE.getOutputCols(),outputCol=\"features\")\n",
    "pipeline=Pipeline(stages=SI+[OHE,VA]).fit(Titanic)\n",
    "Titanic=pipeline.transform(Titanic)\n",
    "# Now transform the train and data sets\n",
    "titanic_train=pipeline.transform(titanic_train)\n",
    "titanic_test=pipeline.transform(titanic_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we have done all the necessary transformations that need  the Machine Learning algorithms, since we have prepared the data as the model required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF=RandomForestClassifier(labelCol=\"survived\", featuresCol=\"features\", numTrees=10).fit(titanic_train)\n",
    "predictions_RF=RF.transform(titanic_test)\n",
    "evaluator=BinaryClassificationEvaluator(labelCol=\"survived\",metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC Curve is: 0.8431495130297444\n"
     ]
    }
   ],
   "source": [
    "print(\"Area Under ROC Curve is:\",str(evaluator.evaluate(predictions_RF)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important: need to cast to float type, and order by prediction, else it won't work\n",
    "preds_and_labels = predictions.select(['prediction','survived']).withColumn('label', f.col('survived').cast(FloatType())).orderBy('prediction')\n",
    "\n",
    "#select only prediction and label columns\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "\n",
    "#print(metrics.ConfusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[187.,  37.],\n",
       "       [ 45., 119.]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC=LinearSVC(labelCol=\"survived\",featuresCol=\"features\",maxIter=10,regParam=0.5).fit(titanic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=SVC.transform(titanic_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC Curve is: 0.8216599520905928\n"
     ]
    }
   ],
   "source": [
    "print(\"Area Under ROC Curve is:\",str(evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the AUC is worse than RandomForest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logistic_Regression_model=LogisticRegression(featuresCol=\"features\",labelCol=\"survived\",\n",
    "                                            standardization=True).fit(titanic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=Logistic_Regression_model.transform(titanic_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC Curve is: 0.8431495130297444\n"
     ]
    }
   ],
   "source": [
    "print(\"Area Under ROC Curve is:\",str(evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we're going to implement Gradient-Boosted Trees algorithm for classfification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBT_model=GBTClassifier(labelCol=\"survived\",minInstancesPerNode=3).fit(titanic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_GBT=GBT_model.transform(titanic_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC Curve is: 0.8625460647538823\n"
     ]
    }
   ],
   "source": [
    "print(\"Area Under ROC Curve is:\",str(evaluator.evaluate(predictions_GBT)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly the GBTClssifier has achieved a better AUC metric than the Random Forest algorithm, and the others did a great job too; but now we're going too choose one of the best of them by using recall(TPR or sensitivity),specificity and the F-Score metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions):\n",
    "    #important: need to cast to float type, and order by prediction, else it won't work\n",
    "    preds_and_labels = predictions.select(['prediction','survived']).withColumn('label', f.col('survived').cast(FloatType())).orderBy('prediction')\n",
    "    #select only prediction and label columns\n",
    "    preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "    metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "    U=metrics.confusionMatrix().toArray()\n",
    "    Recall=U[1,1]/sum(U[1,])\n",
    "    specificity=U[0,0]/sum(U[0,])\n",
    "    Precision=U[1,1]/sum(U[:,1])\n",
    "    print(\"The Recall is:\")\n",
    "    print(Recall)\n",
    "    print(\"The Specificity is:\")\n",
    "    print(specificity)\n",
    "    print(\"The precision is:\")\n",
    "    print(Precision)\n",
    "    print(\"The F-Score is:\")\n",
    "    print((2*Precision*Recall/(Precision+Recall)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Recall is:\n",
      "0.7022900763358778\n",
      "The Specificity is:\n",
      "0.9008620689655172\n",
      "The precision is:\n",
      "0.8\n",
      "The F-Score is:\n",
      "0.7479674796747967\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(predictions_GBT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Recall is:\n",
      "0.5343511450381679\n",
      "The Specificity is:\n",
      "0.9439655172413793\n",
      "The precision is:\n",
      "0.8433734939759037\n",
      "The F-Score is:\n",
      "0.6542056074766355\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(predictions_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the million question is: Wich one do you prefer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would say it depends. If we are more interested in identify the people who are more likely\n",
    "of survived I choose the Random Forest, if it is the opposite I should prefer the GBT.\n",
    "But if we take atention to the F-score, wich merge the two metric I should prefer the\n",
    "GBTClassfier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
